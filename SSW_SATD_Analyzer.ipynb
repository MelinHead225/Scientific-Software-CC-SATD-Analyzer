{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MelinHead225/Scientific-Software-SATD-Analyzer/blob/main/SSW_SATD_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: https://github.com/healpy/healpy.git\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in /healpy: 100%|██████████| 15/15 [00:00<00:00, 7872.19it/s]\n",
      "Processing files in /healpy/bin: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Processing files in /healpy/lib: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/lib/healpy: 100%|██████████| 12/12 [00:00<00:00, 406.72it/s]\n",
      "Processing files in /healpy/lib/healpy/utils: 100%|██████████| 2/2 [00:00<00:00, 3298.71it/s]\n",
      "Processing files in /healpy/lib/healpy/data: 100%|██████████| 30/30 [00:00<00:00, 466033.78it/s]\n",
      "Processing files in /healpy/.git: 100%|██████████| 5/5 [00:00<00:00, 97997.76it/s]\n",
      "Processing files in /healpy/.git/branches: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/refs: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/refs/tags: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/refs/heads: 100%|██████████| 1/1 [00:00<00:00, 26546.23it/s]\n",
      "Processing files in /healpy/.git/refs/remotes: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/refs/remotes/origin: 100%|██████████| 1/1 [00:00<00:00, 24244.53it/s]\n",
      "Processing files in /healpy/.git/logs: 100%|██████████| 1/1 [00:00<00:00, 19691.57it/s]\n",
      "Processing files in /healpy/.git/logs/refs: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/logs/refs/heads: 100%|██████████| 1/1 [00:00<00:00, 22310.13it/s]\n",
      "Processing files in /healpy/.git/logs/refs/remotes: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/logs/refs/remotes/origin: 100%|██████████| 1/1 [00:00<00:00, 23696.63it/s]\n",
      "Processing files in /healpy/.git/hooks: 100%|██████████| 13/13 [00:00<00:00, 184833.74it/s]\n",
      "Processing files in /healpy/.git/objects: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/objects/pack: 100%|██████████| 2/2 [00:00<00:00, 25343.23it/s]\n",
      "Processing files in /healpy/.git/objects/info: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/.git/info: 100%|██████████| 1/1 [00:00<00:00, 20164.92it/s]\n",
      "Processing files in /healpy/.github: 100%|██████████| 2/2 [00:00<00:00, 42581.77it/s]\n",
      "Processing files in /healpy/.github/workflows: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "Processing files in /healpy/src: 100%|██████████| 12/12 [00:00<00:00, 2770.04it/s]\n",
      "Processing files in /healpy/test: 100%|██████████| 14/14 [00:00<00:00, 1546.08it/s]\n",
      "Processing files in /healpy/test/data: 100%|██████████| 20/20 [00:00<00:00, 276851.75it/s]\n",
      "Processing files in /healpy/doc: 100%|██████████| 24/24 [00:00<00:00, 13311.73it/s]\n",
      "Processing files in /healpy/doc/templates: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/doc/templates/autosummary: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Processing files in /healpy/doc/static: 100%|██████████| 4/4 [00:00<00:00, 93727.46it/s]\n",
      "Processing files in /healpy/cextern: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/cextern/cfitsio: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/cextern/healpix: 0it [00:00, ?it/s]\n",
      "Processing files in /healpy/paper: 100%|██████████| 3/3 [00:00<00:00, 48770.98it/s]\n",
      "Preprocessing comments: 100%|██████████| 544/544 [00:00<00:00, 25001.93it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172f4122e684259898b0589bc875d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eabc782edd74bf68c4b376eee7d4874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaf741eb7ec4e9d8ba414a691714b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0150e6f302d4680b373f60001ed44e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e43a239a39c4dd28bab06a6eb19f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/960 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa84211d8316413d9a6e6174a049f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying comments: 100%|██████████| 544/544 [00:12<00:00, 45.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified comments saved to healpy_comments_classification.csv\n",
      "\n",
      "=== Project-Level SATD Analysis ===\n",
      "Repository: healpy\n",
      "Total Comments Analyzed: 544\n",
      "SATD Comments: 177 (32.54%)\n",
      "\n",
      "SATD Distribution:\n",
      "non_debt: 367 (67.46%)\n",
      "test_debt: 1 (0.18%)\n",
      "scientific_debt: 171 (31.43%)\n",
      "code/design_debt: 4 (0.74%)\n",
      "requirement_debt: 1 (0.18%)\n",
      "\n",
      "Top 5 Files with Most SATD:\n",
      "lib/healpy/pixelfunc.py: 40 SATD comments\n",
      "lib/healpy/sphtfunc.py: 23 SATD comments\n",
      "lib/healpy/rotator.py: 22 SATD comments\n",
      "lib/healpy/projaxes.py: 13 SATD comments\n",
      "src/_sphtools.pyx: 12 SATD comments\n",
      "\n",
      "Top 5 Directories with Most SATD:\n",
      "lib/healpy: 132 SATD comments\n",
      "src: 23 SATD comments\n",
      "test: 18 SATD comments\n",
      "lib/healpy/utils: 3 SATD comments\n",
      "root: 1 SATD comments\n",
      "\n",
      "SATD Distribution by File Type:\n",
      ".py: 154 SATD comments\n",
      ".pyx: 23 SATD comments\n",
      "\n",
      "Deleted cloned repository: /healpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-1-2d41bf486a6f>:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  satd_df[\"Directory\"] = satd_df[\"Filename\"].apply(lambda x: os.path.dirname(x) if os.path.dirname(x) else \"root\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import git\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import shutil\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Label mapping for classification\n",
    "LABEL_MAPPING = {\n",
    "    0: \"requirement_debt\",\n",
    "    1: \"code/design_debt\",\n",
    "    2: \"documentation_debt\",\n",
    "    3: \"test_debt\",\n",
    "    4: \"scientific_debt\",\n",
    "    5: \"non_debt\"\n",
    "}\n",
    "\n",
    "# Clone GitHub repository\n",
    "def clone_github_repo(repo_url, local_dir):\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "    git.Repo.clone_from(repo_url, local_dir)\n",
    "\n",
    "# Preprocess comments: Convert multi-line comments to single-line & clean text\n",
    "def preprocess_comments(comments):\n",
    "    def multiline_to_singleline(comment):\n",
    "        return ' '.join(comment.splitlines())\n",
    "\n",
    "    def clean_comment(comment):\n",
    "        comment = re.sub(r'\\s*(/\\*\\*|\\*/|/\\*|//)', '', comment)\n",
    "        comment = re.sub(r'[^a-zA-Z!?\\s]', '', comment)\n",
    "        comment = comment.lower()\n",
    "        comment = re.sub(r'\\s+', ' ', comment).strip()\n",
    "        return comment\n",
    "\n",
    "    processed_comments = []\n",
    "    for filename, line_number, comment in tqdm(comments, desc=\"Preprocessing comments\"):\n",
    "        if comment.startswith('/**') or comment.startswith('/*'):\n",
    "            comment = multiline_to_singleline(comment)\n",
    "        comment = clean_comment(comment)\n",
    "        if comment:\n",
    "            processed_comments.append((filename, line_number, comment))\n",
    "    return processed_comments\n",
    "\n",
    "# Extract comments from a single file\n",
    "def extract_comments_from_file(file_path):\n",
    "    comments = []\n",
    "    encodings = ['utf-8', 'latin-1', 'utf-16']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                lines = file.readlines()\n",
    "                inside_multiline_comment = False\n",
    "                multiline_comment = \"\"\n",
    "                current_comment = \"\"\n",
    "                line_number = 0\n",
    "                file_extension = os.path.splitext(file_path)[1]\n",
    "\n",
    "                for line in lines:\n",
    "                    line_number += 1\n",
    "                    if file_extension in ('.py', '.pyx'):\n",
    "                        match_single = re.match(r'^\\s*#(.*)', line)\n",
    "                        match_multi_start = re.match(r'^\\s*(?:\"\"\"|\\'\\'\\')(.*)', line)\n",
    "                        match_multi_end = re.match(r'(.*)(?:\"\"\"|\\'\\'\\')\\s*$', line)\n",
    "                        multiline_comment_end = '\"\"\"'\n",
    "                    elif file_extension == '.go':\n",
    "                        match_single = re.match(r'^\\s*//(.*)', line)\n",
    "                        match_multi_start = re.match(r'^\\s*/\\*(.*)', line)\n",
    "                        match_multi_end = re.match(r'(.*)\\*/\\s*$', line)\n",
    "                        multiline_comment_end = '*/'\n",
    "                    elif file_extension in ('.c', '.cpp', '.h', '.hpp', '.php', '.phtml', '.js', '.jsx', '.ts', '.tsx'):\n",
    "                        match_single = re.match(r'^\\s*//(.*)', line)\n",
    "                        match_multi_start = re.match(r'^\\s*/\\*(.*)', line)\n",
    "                        match_multi_end = re.match(r'(.*)\\*/\\s*$', line)\n",
    "                        multiline_comment_end = '*/'\n",
    "                    elif file_extension in ('.pl', '.pm'):\n",
    "                        match_single = re.match(r'^\\s*#(.*)', line)\n",
    "                        match_multi_start = match_multi_end = None\n",
    "                        multiline_comment_end = None\n",
    "                    elif file_extension in ('.f', '.for', '.f90'):\n",
    "                        match_single = re.match(r'^\\s*[!C].?(.*)', line)\n",
    "                        match_multi_start = match_multi_end = None\n",
    "                        multiline_comment_end = None\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if match_single:\n",
    "                        comment = match_single.group(1).strip()\n",
    "                        if comment:\n",
    "                            if current_comment:\n",
    "                                current_comment += \" \" + comment\n",
    "                            else:\n",
    "                                current_comment = comment\n",
    "                        continue\n",
    "\n",
    "                    if not inside_multiline_comment and match_multi_start:\n",
    "                        inside_multiline_comment = True\n",
    "                        multiline_comment = match_multi_start.group(1).strip()\n",
    "                        if multiline_comment_end and multiline_comment.endswith(multiline_comment_end):\n",
    "                            multiline_comment = multiline_comment[:-len(multiline_comment_end)].strip()\n",
    "                            if multiline_comment:\n",
    "                                if current_comment:\n",
    "                                    current_comment += \" \" + multiline_comment\n",
    "                                else:\n",
    "                                    current_comment = multiline_comment\n",
    "                            inside_multiline_comment = False\n",
    "                        continue\n",
    "\n",
    "                    if inside_multiline_comment:\n",
    "                        multiline_comment += \" \" + line.strip()\n",
    "                        if multiline_comment_end and multiline_comment.endswith(multiline_comment_end):\n",
    "                            multiline_comment = multiline_comment[:-len(multiline_comment_end)].strip()\n",
    "                            if multiline_comment:\n",
    "                                if current_comment:\n",
    "                                    current_comment += \" \" + multiline_comment\n",
    "                                else:\n",
    "                                    current_comment = multiline_comment\n",
    "                            inside_multiline_comment = False\n",
    "                        continue\n",
    "\n",
    "                    if current_comment:\n",
    "                        comments.append((line_number, current_comment))\n",
    "                        current_comment = \"\"\n",
    "\n",
    "                if current_comment:\n",
    "                    comments.append((line_number, current_comment))\n",
    "            return comments\n",
    "        except (UnicodeDecodeError, IOError):\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"Unable to decode the file {file_path} with available encodings.\")\n",
    "\n",
    "# Traverse directory and extract comments (includes full file path)\n",
    "def traverse_directory_and_extract_comments(root_dir):\n",
    "    all_comments = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in tqdm(filenames, desc=f\"Processing files in {dirpath}\"):\n",
    "            if filename.endswith(('.py', '.pyx', '.go', '.f', '.for', '.f90', '.c', '.cpp', '.h', '.hpp', '.pl', '.pm', '.php', '.phtml', '.js', '.jsx', '.ts', '.tsx')):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                comments = extract_comments_from_file(file_path)\n",
    "                for line_number, comment in comments:\n",
    "                    relative_path = os.path.relpath(file_path, root_dir)\n",
    "                    all_comments.append((relative_path, line_number, comment))\n",
    "    return all_comments\n",
    "\n",
    "# Classify comments using Hugging Face model\n",
    "def classify_comment(comment, model, tokenizer):\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        label = torch.argmax(probs, dim=-1).item()\n",
    "    return LABEL_MAPPING[label]\n",
    "\n",
    "HF_REPO_NAME = \"MelinHead225/bert-large-SSW-SATD-classification\"\n",
    "\n",
    "# Project-level SATD analysis (text-only)\n",
    "def perform_project_level_analysis(df, repo_name):\n",
    "    class_counts = Counter(df[\"Classification\"])\n",
    "    total_comments = len(df)\n",
    "    satd_comments = total_comments - class_counts.get(\"non_debt\", 0)\n",
    "    satd_percentage = (satd_comments / total_comments * 100) if total_comments > 0 else 0\n",
    "\n",
    "    print(\"\\n=== Project-Level SATD Analysis ===\")\n",
    "    print(f\"Repository: {repo_name}\")\n",
    "    print(f\"Total Comments Analyzed: {total_comments}\")\n",
    "    print(f\"SATD Comments: {satd_comments} ({satd_percentage:.2f}%)\")\n",
    "    print(\"\\nSATD Distribution:\")\n",
    "    for class_label, count in class_counts.items():\n",
    "        print(f\"{class_label}: {count} ({count/total_comments*100:.2f}%)\")\n",
    "\n",
    "    satd_df = df[df[\"Classification\"] != \"non_debt\"]\n",
    "    file_satd_counts = satd_df[\"Filename\"].value_counts().head(5)\n",
    "    print(\"\\nTop 5 Files with Most SATD:\")\n",
    "    for file, count in file_satd_counts.items():\n",
    "        print(f\"{file}: {count} SATD comments\")\n",
    "\n",
    "    satd_df[\"Directory\"] = satd_df[\"Filename\"].apply(lambda x: os.path.dirname(x) if os.path.dirname(x) else \"root\")\n",
    "    dir_satd_counts = satd_df[\"Directory\"].value_counts().head(5)\n",
    "    print(\"\\nTop 5 Directories with Most SATD:\")\n",
    "    for directory, count in dir_satd_counts.items():\n",
    "        print(f\"{directory}: {count} SATD comments\")\n",
    "\n",
    "    filetype_counts = Counter()\n",
    "    for filename in satd_df[\"Filename\"]:\n",
    "        ext = os.path.splitext(filename)[1]\n",
    "        filetype_counts[ext] += 1\n",
    "    print(\"\\nSATD Distribution by File Type:\")\n",
    "    for ext, count in filetype_counts.most_common():\n",
    "        print(f\"{ext}: {count} SATD comments\")\n",
    "\n",
    "# Save comments and perform analysis\n",
    "def save_results_to_csv(comments, output_file, repo_name):\n",
    "    df = pd.DataFrame(comments, columns=['Filename', 'Line Number', 'Comment'])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_REPO_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(HF_REPO_NAME).to(device)\n",
    "\n",
    "    tqdm.pandas(desc=\"Classifying comments\")\n",
    "    df[\"Classification\"] = df[\"Comment\"].progress_apply(lambda c: classify_comment(c, model, tokenizer))\n",
    "\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nClassified comments saved to {output_file}\")\n",
    "\n",
    "    perform_project_level_analysis(df, repo_name)\n",
    "\n",
    "def main():\n",
    "    REPO_URLS = [\n",
    "        # \"analyse_this_repository.git\", # This is an example. Insert actual repository here    #\n",
    "        \"https://github.com/healpy/healpy.git\"\n",
    "    ]\n",
    "    BASE_DIR = \"/\"\n",
    "\n",
    "    for REPO_URL in REPO_URLS:\n",
    "        print(f\"\\nProcessing repository: {REPO_URL}\")\n",
    "\n",
    "        repo_name = REPO_URL.split('/')[-1].replace('.git', '')\n",
    "        OUTPUT_FILE = f\"{repo_name}_comments_classification.csv\"\n",
    "        LOCAL_DIR = os.path.join(BASE_DIR, repo_name)\n",
    "\n",
    "        clone_github_repo(REPO_URL, LOCAL_DIR)\n",
    "        comments = traverse_directory_and_extract_comments(LOCAL_DIR)\n",
    "        processed_comments = preprocess_comments(comments)\n",
    "        save_results_to_csv(processed_comments, OUTPUT_FILE, repo_name)\n",
    "        shutil.rmtree(LOCAL_DIR)\n",
    "        print(f\"\\nDeleted cloned repository: {LOCAL_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
